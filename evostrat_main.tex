% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%


\documentclass[runningheads,20pt]{llncs}


% \documentclass[14pt]{extarticle} % also accepts 17pt, 20pt
\usepackage[T1]{fontenc}
% \usepackage[top=0.5in,bottom=0.5in,left=0.5in,right=0.5in]{geometry}


\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{stfloats} % for positioning of figure* on the same page
\usepackage{caption}
\usepackage{tikz}
\usepackage[inline]{enumitem}
\usepackage[breaklinks=true,colorlinks=true, allcolors=blue]{hyperref}
\usepackage{breakcites}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{xcolor}
\usepackage{array}
\usepackage{float}
\usepackage{adjustbox}
\usepackage{listings}
\usepackage{csquotes}
\usepackage{makecell}
\usepackage{pdfpages}
\usepackage{xspace}
\usepackage{tikz}

\usetikzlibrary{positioning,shapes.geometric}

% to do:
% hearing tests
% one other way to present results is grouped by which losses are most appropriate for each situtation

\input{code_style}

\captionsetup[lstlisting]{justification=centering, singlelinecheck=false}
\providecommand{\gls}[1]{#1}
\newcommand{\highlight}[1]{\textcolor[RGB]{00,150,00}{#1}}
\newcommand{\todo}[1]{\textcolor{red}{#1}}

\newcommand{\SIMSESpec}{\texttt{SIMSE\_Spec}\xspace}
\newcommand{\LoneSpec}{\texttt{L1\_Spec}\xspace}
\newcommand{\JTFS}{\texttt{JTFS}\xspace}
\newcommand{\DTWEnv}{\texttt{DTW\_Envelope}\xspace}
\newcommand{\OutDomain}{\textbf{Out-of-Domain Generation}\xspace}
\newcommand{\LossSelect}{\textbf{Loss Selection}}
\newcommand{\SynthSelect}{\textbf{Synthesis Selection}}
\newcommand{\PeriodicLoss}{\textbf{Periodic Loss}}

\newcommand{\BPNoise}{\textbf{BP-Noise}\xspace}
\newcommand{\BPSaw}{\textbf{BP-Saw}\xspace}
\newcommand{\AddSineSaw}{\textbf{Add-SineSaw}\xspace}
\newcommand{\AmpMod}{\textbf{Noise-AM}\xspace}
\newcommand{\FMMod}{\textbf{SineSaw-AM}\xspace}
\newcommand{\FMModvtwo}{\textbf{SineSine-AM}\xspace}
\newcommand{\PitchBendUp}{\textbf{PitchBend-Up}\xspace}

\begin{document}

\begin{abstract}
 Out-of-domain sound-matching is the task of automatically programming a synthesizer towards a sound that it cannot accurately replicate. Measuring performance in out-of-domain sound-matching is a difficult task to the subjective experience of sound, open-set recognition, characteristics of interest, etc. In addition, despite their critical role in sound-matching, the performance of different sound-similarity measures (or loss functions) under different circumstances has rarely been a topic of research. Should we be looking for a global sound-similarity measure, or is the choice of loss function a creative decision, much like the selection of a synthesizer?
 Here we present a series of differentiable out-of-domain sound-matching scenarios using four loss functions and various synthesizers. The experiments here are designed such that differences in parameters (whether all parameters or a subset) are well suited for measuring performance in sound-matching. The out-of-domain experiments here showcase the characteristics of the different loss functions, and confirm that their success is highly dependent on the method of synthesis and the target sound. 
\end{abstract}

\section{Introduction}
\label{sec:intro}
Manual sound design with a synthesizer is inherently iterative: an artist compares the synthesized output to a mental or auditory target, adjusts parameters, and repeats until the desired result is achieved~\cite{salimi2025soundmatching}.  
Differentiable iterative sound-matching automates this process by optimizing synthesizer parameters under the guidance of a loss function that measures similarity between a target and an output signal.  
Recent work has demonstrated that loss–synthesizer interactions are highly dependent on the synthesis method, with no universally optimal loss function across domains~\cite{salimi2025soundmatching}.  
That study established a systematic evaluation framework combining differentiable synthesizers, loss functions, and statistical ranking to reveal how different metrics succeed or fail depending on synthesis structure.

However, nearly all prior studies—including ours—assume an \emph{in-domain} setting, where the target and imitator synthesizers share identical architectures and parameterizations.  
This assumption simplifies evaluation but fails to represent real sound-design workflows.  
In practice, designers frequently attempt to imitate sounds created with other synthesizers, physical instruments, or recorded sources.  
Such settings define the \emph{out-of-domain} (OOD) sound-matching problem, where exact replication of the target signal is impossible and the goal shifts from \emph{parameter recovery} to \emph{perceptual imitation}.  
Despite its importance to creative audio practice, OOD sound-matching has received little systematic analysis and remains poorly understood.

In OOD conditions, only a subset of parameters can meaningfully correspond between the target and imitator.  
For instance, when matching a band-pass filtered saw wave with a noise synthesizer, the absolute pitch or harmonic content of the signal cannot align, yet the filter cutoffs can.  
To capture success in such scenarios, we introduce the \emph{partial parameter loss} (PPL), which measures the distance only over the critical subset of parameters expected to transfer perceptually between the target and imitator.  
PPL provides an interpretable performance measure for OOD experiments, enabling quantitative comparison even when large portions of the parameter space are unmatched or undefined.

Using the same differentiable iterative framework as in~\cite{salimi2025soundmatching}, we evaluate four loss functions—L1\_Spec, SIMSE\_Spec, DTW\_Envelope, and JTFS—across several controlled OOD scenarios designed to probe specific perceptual dimensions: (1) \textbf{band-pass filtering}, where cutoff frequencies are the critical features; (2) \textbf{amplitude modulation}, where modulation rates must align but carrier frequencies may not; and (3) \textbf{pitch-bending}, where temporal frequency trajectories differ by onset or curvature.  
For each scenario, we record 300 trials per loss function and analyze the resulting distributions of PPL values using bootstrapped ranking and the non-parametric Scott–Knott test.

\textbf{Objectives and Hypotheses.}  
This study extends differentiable iterative sound-matching into the OOD regime and tests the following hypotheses:
\begin{itemize}
  \item \textbf{H1:} Loss–synthesizer interactions observed in in-domain settings persist under OOD conditions, but with greater performance variability due to mismatched timbral attributes.
  \item \textbf{H2:} Context-sensitive losses such as DTW\_Envelope and SIMSE\_Spec yield higher PPL performance in OOD tasks where temporal or spectral envelopes are the dominant perceptual cues.
  \item \textbf{H3:} Agreement between automatic metrics and perceptual outcomes decreases under OOD conditions, emphasizing the need for feature-selective evaluation methods.
\end{itemize}
Together, these hypotheses aim to reveal whether loss functions can generalize beyond the boundaries of their training or design domain, and to identify which similarity measures best capture perceptual alignment when perfect reconstruction is unattainable.

\section{Background and Related Work}
\label{sec:background}

Sound-matching is the process of automatically programming a synthesizer to reproduce or imitate a target sound.  
Formally, it involves a parametric synthesizer $g(\theta)$, a target signal $x_0$, a representation function $\phi(\cdot)$, and a similarity measure $L(\theta, x_0) = d(\phi(g(\theta)), \phi(x_0))$, where $d$ denotes a distance metric.  
The objective is to find parameter settings $\theta^*$ that minimize $L$, yielding a synthesized output perceptually close to the target.  
This framework underlies both traditional and differentiable sound-matching approaches.

\subsection{In-Domain Sound-Matching}
Historically, sound-matching has been studied under \emph{in-domain} conditions, where the target and imitator synthesizers are identical.  
Early works relied on non-differentiable heuristics such as genetic algorithms and hill-climbers~\cite{horner1993machine,mitchell2007evolutionary}, which iteratively searched parameter spaces using hand-crafted spectral or cepstral metrics.  
Subsequent approaches introduced supervised neural models that learned mappings between synthesizer parameters and audio outputs~\cite{yee2018auto,esling2019flow,masuda2021synthesizer}.  
While these methods achieved faster convergence, they largely assumed access to paired datasets generated by the same synthesizer, limiting their applicability to cross-domain scenarios.

Recent research has revisited sound-matching through the lens of differentiable digital signal processing (DDSP)~\cite{engel2020ddsp}, where synthesizer modules and losses are implemented as differentiable operations.  
This has enabled gradient-based optimization of parameters directly through audio signals and feature representations.  
Within this framework, differentiable losses have included parameter losses (P-Loss), spectral losses based on short-time Fourier transforms (STFT or MSS), and more recent alternatives such as the joint time–frequency scattering (JTFS) transform~\cite{vahidi2023mesostructures} and differentiable dynamic-time-warping losses~\cite{cuturi2017softdtw}.  
These measures have shown distinct strengths depending on synthesis method—e.g., spectrogram-based losses for subtractive synthesis and temporal-alignment losses for modulated tones—yet all were validated in in-domain contexts.

\subsection{Toward Out-of-Domain Sound-Matching}
The transition from in-domain to \emph{out-of-domain} (OOD) settings introduces a fundamental challenge: the target sound may not be realizable by the chosen synthesizer.  
As noted by Masuda et al.~\cite{masuda2021synthesizer} and Uzrad et al.~\cite{uzrad2024diffmoog}, OOD scenarios break the assumption of parameter correspondence, requiring the imitator to approximate perceptual rather than structural similarity.  
OOD sound-matching therefore shifts the goal from replication to imitation—seeking partial alignment on perceptually salient features such as amplitude envelopes or spectral balance.  
Despite its relevance, systematic OOD evaluation remains scarce, and existing works often conflate OOD generation with data augmentation or domain adaptation rather than explicit cross-synth imitation.

The evaluation of OOD success further complicates the problem.  
Metrics such as spectral L1 or mean-squared error are sensitive to global gain and timbral differences that may be irrelevant to perceived similarity.  
Even advanced feature-space losses like JTFS~\cite{vahidi2023mesostructures} have shown inconsistent behavior when temporal or spectral structures are mismatched~\cite{salimi2025soundmatching}.  
Few studies have attempted to isolate which aspects of the signal should be compared when full waveform alignment is unattainable.

\subsection{Gaps and Motivation}
Three gaps in the literature motivate the present work.  
First, there is no standardized methodology for evaluating sound-matching when the target cannot be exactly synthesized by the imitator.  
Second, loss-function generalization across synthesis domains is poorly understood: functions optimized for a specific architecture often fail under structural mismatch.  
Third, objective metrics seldom capture the partial perceptual success of imitation tasks.

To address these limitations, we adopt a differentiable iterative framework similar to~\cite{salimi2025soundmatching} but extend it to OOD scenarios where the target and imitator synthesizers differ in architecture or parameter range.  
We evaluate the ability of four differentiable loss functions—L1\_Spec, SIMSE\_Spec, DTW\_Envelope, and JTFS—to generalize under controlled cross-domain conditions.  
By introducing the partial parameter loss (PPL) as an evaluation metric, we aim to quantify how well each loss function preserves the critical transferable parameters of the target while tolerating unavoidable mismatches in other attributes.

This focus on controlled OOD experimentation and selective evaluation distinguishes the present work from prior DDSP studies and positions it as a step toward more generalizable, perceptually grounded sound-matching systems.


\bibliographystyle{alpha}
\bibliography{references}

\end{document}


