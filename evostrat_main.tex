
% This paper compares the performance of loss functions previously not tested in OOD settings, and verifies their utility. We define sound-matching tasks with targets that cannot be matched by the synthesizer. Each target sound is from a target synthesizer which has at least 1 critical parameter in common with the output synthesizer. The performance measure of interest here is how close can we approximate the difference in the set of critical parameters between the target and output synth. We call this proximity the partial parameter loss (PPL), and define it such that it can be taken as a reasonable measure of performance in each sound-matching test. 
% we conduct a series of OOD tests to measure both whether there is a variation in performance of loss functions depending on the synthesizer, and verify the strength of said loss functions (spectrogram losses excel at filtering, DTW_env excells at amplitude envelope modulations, and JTFS is useful for delayed chirplets.


\documentclass[runningheads]{llncs}


% \documentclass[14pt]{extarticle} % also accepts 17pt, 20pt
% \usepackage[top=0.5in,bottom=0.5in,left=0.5in,right=0.5in]{geometry}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{stfloats} % for positioning of figure* on the same page
\usepackage{caption}
\usepackage{tikz}
\usepackage[inline]{enumitem}
\usepackage[breaklinks=true,colorlinks=true, allcolors=blue]{hyperref}
\usepackage{breakcites}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{xcolor}
\usepackage{array}
\usepackage{float}
\usepackage{adjustbox}
\usepackage{listings}
\usepackage{csquotes}
\usepackage{makecell}
\usepackage{pdfpages}
\usepackage{xspace}
\usepackage{tikz}

\usetikzlibrary{positioning,shapes.geometric}

% to do:
% hearing tests
% one other way to present results is grouped by which losses are most appropriate for each situtation

\input{code_style}

\captionsetup[lstlisting]{justification=centering, singlelinecheck=false}
\providecommand{\gls}[1]{#1}
\newcommand{\highlight}[1]{\textcolor[RGB]{00,150,00}{#1}}
\newcommand{\todo}[1]{\textcolor{red}{#1}}

\newcommand{\SIMSESpec}{\texttt{SIMSE\_Spec}\xspace}
\newcommand{\LoneSpec}{\texttt{L1\_Spec}\xspace}
\newcommand{\JTFS}{\texttt{JTFS}\xspace}
\newcommand{\DTWEnv}{\texttt{DTW\_Envelope}\xspace}
\newcommand{\OutDomain}{\textbf{Out-of-Domain Generation}\xspace}
\newcommand{\LossSelect}{\textbf{Loss Selection}}
\newcommand{\SynthSelect}{\textbf{Synthesis Selection}}
\newcommand{\PeriodicLoss}{\textbf{Periodic Loss}}

\newcommand{\BPNoise}{\textbf{BP-Noise}\xspace}
\newcommand{\BPSaw}{\textbf{BP-Saw}\xspace}
\newcommand{\AddSineSaw}{\textbf{Add-SineSaw}\xspace}
\newcommand{\AmpMod}{\textbf{Noise-AM}\xspace}
\newcommand{\FMMod}{\textbf{SineSaw-AM}\xspace}
\newcommand{\FMModvtwo}{\textbf{SineSine-AM}\xspace}
\newcommand{\PitchBendUp}{\textbf{PitchBend-Up}\xspace}

\begin{document}


\title{Out-of-Domain Evaluation of Sound Similarity Metrics in Iterative, Differentiable Sound-Matching}
%
\titlerunning{Out-of-Domain Evaluation of Sound Similarity Metrics in Sound-Matching}

\author{Amir Salimi\inst{1}\orcidID{0000-0001-5916-1050
} \and
Abram Hindle\inst{1}\orcidID{0000-0002-4373-4958} \and
Osmar R. Zaïane\inst{1,2}\orcidID{0000-0002-0060-5988}}
%
\authorrunning{A. Salimi et al.}
%
\institute{
Department of Computing Science, University of Alberta, Edmonton, Canada\\
\email{\{asalimi, abram.hindle, zaiane\}@ualberta.ca}
\and
Alberta Machine Intelligence Institute (Amii),\\
10065 Jasper Ave \#1101, Edmonton, AB T5J 1S5, Canada
}

\maketitle              % typeset the header of the contribution
%

\begin{abstract}
Out-of-domain (OOD) sound-matching is the task of automatically programming a synthesizer toward a target sound that it cannot fully reproduce. 
Measuring success in this setting remains underexplored due to the subjective nature of sound perception and the problem of open set recognition in the auditory domain. 
Yet OOD sound-matching more accurately reflects the challenges faced by sound designers, for whom exact parameter recovery is rarely possible. 
Building on prior in-domain findings that the performance of loss functions depends on the synthesis method, this study extends the analysis to OOD conditions. 
We design a series of controlled experiments where each target and imitator synthesizer pair shares at least one critical parameter, enabling objective evaluation through a \emph{partial parameter loss} (PPL) that measures proximity between the corresponding parameter subsets. 
Across multiple scenarios, we compare four differentiable loss functions—L1\_Spec, SIMSE\_Spec, JTFS, and DTW\_Envelope—under filtering, amplitude modulation, and pitch-bending conditions. 
Results show that spectrogram-based losses excel in filter imitation, DTW\_Envelope captures envelope-modulated dynamics most effectively, and JTFS is beneficial when temporal alignment is preserved. 
These findings verify that loss-function performance remains highly dependent on synthesis context, even under out-of-domain constraints.
\end{abstract}

\section{Introduction}
\label{sec:intro} 

Sound-matching is the task of automatically programming a parameterized audio synthesizer so that its output approximates a target sound, with similarity measured by a loss function or similarity metric.
In-domain sound-matching refers to cases where the target sound is itself produced by the same synthesizer, allowing direct parameter correspondence and evaluation through accurate replication.
By contrast, out-of-domain (OOD) sound-matching involves targets that cannot be exactly reproduced—such as sounds generated by other synthesizers, acoustic instruments, or recordings—making the task one of perceptual imitation rather than replication.
Although most sound-matching research is motivated by assisting artists in creative sound design, the overwhelming majority of prior studies focus exclusively on in-domain scenarios, which poorly reflect the realities of artistic workflows.
In particular, there remains little systematic understanding of how different similarity measures interact with different synthesis methods.

Recent work~\cite{salimi2025evaluating} demonstrated that, even in in-domain conditions, the interaction between loss functions and synthesizer architecture is highly dependent on the synthesis method—no single similarity metric performs optimally across all synthesizers.
These findings highlight the need for more exploratory and context-aware approaches to loss design, rather than reliance on “state-of-the-art” (SOTA) metrics as universal solutions.
As a step toward automated interpretive sound design, this study investigates whether the optimal choice of a similarity measure depends on the synthesis method under out-of-domain (OOD) conditions, and examines the strengths and weaknesses of lesser-used losses based on Dynamic Time Warping (DTW) and Scale-Invariant Mean Squared Error (SIMSE).

Evaluating performance in OOD settings is particularly challenging, as only a subset of parameters between the target and imitator can meaningfully correspond.
For example, when matching a band-pass-filtered saw wave using a noise synthesizer, absolute pitch and harmonic structure cannot align, yet the filter cutoffs can.
To quantify success in such scenarios, we introduce the Partial Parameter Loss (PPL), which measures differences only over the subset of critical parameters expected to transfer perceptually between the target and imitator.
PPL thus provides an interpretable, quantitative measure of performance in OOD sound-matching, even when large portions of the parameter space are unmatched or undefined.
Using PPL as the primary evaluation score, we apply statistical ranking methods to controlled sound-design scenarios to compare the outcomes of different synthesizer–loss combinations.

\section{Background and Related Work}
\label{sec:background}
Since the 1970s, artists such as Suzanne Ciani~\cite{ciani_life_in_waves} have used audio synthesizers to evoke natural sounds—like a bottle cap popping off—in ways that preserve the perceptual essence of the original while highlighting the unique character of electronic synthesis~\cite{creativecherep2024}.
This practice illustrates a broader artistic philosophy: synthesizers are not merely tools for replication, but instruments for reinterpretation.
Yet despite advances in digital synthesis and differentiable optimization, automating this interpretive approach to sound design remains impractical and rarely explored.

Sound-matching is the process of automatically programming a synthesizer to reproduce or imitate a target sound.  
Formally, it involves a parametric synthesizer $g(\theta)$, a target signal $x_0$, a representation function $\phi(\cdot)$, and a similarity measure $L(\theta, x_0) = d(\phi(g(\theta)), \phi(x_0))$, where $d$ denotes a distance metric~\cite{vahidi2023mesostructures,han2023perceptual,salimi2025evaluating}. The objective is to find parameter settings $\theta^*$ that minimize $L$, yielding an output that approximates all or some features of the target sound.  

\subsection{In-Domain Sound-Matching}
Historically, sound-matching has been studied under \emph{in-domain} conditions, where the target and imitator synthesizers are identical.  
Early works relied on non-differentiable heuristics such as genetic algorithms and hill-climbers~\cite{horner1993machine,mitchell2007evolutionary}, which iteratively searched parameter spaces using Fourier based representations of sound as loss functions~\cite{smith2007mathematics}.  
Subsequent approaches introduced supervised neural models that learned mappings between synthesizer parameters and audio outputs~\cite{yee2018automatic,esling2019flow,masuda2021soundmatch}. Such supervised approaches require access to large datasets of paired sounds and programs generated by the same synthesizer, limiting their applicability to cross-domain scenarios.

Recent research has revisited sound-matching through the lens of differentiable digital signal processing (DDSP)~\cite{engel2020ddsp}, where synthesizer modules and losses are implemented as differentiable operations.  
This has enabled gradient-based optimization of parameters directly through audio signals and feature representations.  
Within this framework, differentiable losses have included spectral losses based on short-time Fourier transforms (STFT or MSS), and more recent alternatives such as the joint time–frequency scattering (JTFS) transform~\cite{vahidi2023mesostructures} and differentiable dynamic-time-warping losses~\cite{salimi2025evaluating}.  
These measures have shown distinct strengths depending on synthesis method—e.g., spectrogram-based losses for subtractive synthesis and temporal-alignment losses for modulated tones—yet all were validated in in-domain contexts.

\subsection{Toward Out-of-Domain Sound-Matching}
The transition from in-domain to OOD settings introduces a fundamental challenge: the target sound may not be realizable by the chosen synthesizer. OOD scenarios break the assumption of parameter correspondence, and therefore shift the goal from replication to imitation—seeking partial alignment to a subset of the sounds features. Despite its relevance, systematic OOD evaluation remains scarce~\todo{cite}

The evaluation of OOD success further complicates the problem.  
Metrics such as spectral L1 or mean-squared error are sensitive to global gain and timbral differences that may be irrelevant to perceived similarity~\cite{turian2020sorry,salimi2025evaluating}.  
Even advanced feature-space losses like JTFS~\cite{anden2015joint} have shown inconsistent behavior when temporal or spectral structures are mismatched~\cite{salimi2025soundmatching}.  
Few studies have attempted to isolate which aspects of the signal should be compared when full waveform alignment is unattainable.

\subsection{Gaps and Motivation}
We adopt a differentiable iterative framework similar to~\cite{salimi2025evaluating} but extend it to OOD scenarios where the target and imitator synthesizers differ in architecture or parameter range.  
We evaluate the ability of four differentiable loss functions—L1\_Spec, SIMSE\_Spec, DTW\_Envelope, and JTFS—to generalize under controlled cross-domain conditions.  
By introducing the partial parameter loss (PPL) as an evaluation metric, we aim to quantify how well each loss function preserves the critical transferable parameters of the target while tolerating unavoidable---and in fact, encouraged---mismatches in other attributes.


\subsection{Method Overview}
\label{sec:method_overview}

Our experimental design extends the differentiable iterative sound-matching framework introduced in~\cite{salimi2025evaluating} to OOD scenarios.
Each experiment pairs a target synthesizer $g_t$ and an imitator synthesizer $g_i$, which may differ in structure or parameterization, and optimizes $g_i$ for 200 gradient-based iterations under one of four differentiable loss functions: L1\_Spec, SIMSE\_Spec, JTFS, or DTW\_Envelope. Success in OOD scenario emphasizes partial imitation of perceptually critical parameters that can meaningfully be compared between $g_t$ and $g_i$.

To quantify this partial success, we define a \textit{partial parameter loss} (PPL) as our performance metric.  
PPL measures distance only over the subset of parameters deemed critical for each scenario—for instance, filter cutoffs in band-pass matching or modulation rates in amplitude-modulated synthesis—while ignoring parameters without interpretable correspondence, such as oscillator pitch or waveform shape.  
For each combination of $(g_t, g_i, L)$, 300 trials are conducted to produce a distribution of PPL values.  
The distributions are bootstrapped to 1000 samples and compared using the non-parametric Scott–Knott (\gls{NPSK}) test to obtain ranked performance across loss functions.  

This design enables a systematic investigation of how different differentiable similarity measures generalize beyond their native synthesis domain, and whether their relative strengths—observed previously under in-domain conditions—remain consistent when perfect reproduction of the target is impossible.

\begin{figure}[ht]
    \centering
\begin{tikzpicture}[node distance=2cm, auto]

% Nodes
\node (start) [text centered] {\( g(\hat{\theta}) = x_{\theta} \)};
\node (L) [above of=start, right of=start, text centered] 
    {\( L(\hat{\theta}, x_{0}) \propto \phi(x_{\theta}) - \phi(x_{0}) \)};
\node (optimize) [below of=L, right of=L, text centered] {Optimize $\hat{\theta}$};
\node (new_theta) [below of=optimize, right of=start, text centered] {New \( \hat{\theta} \)};

% Highlight and arrow for the start node
\draw[->, very thick, red] (start) ++(-2,1.5) -- (start)
    node[midway, below, align=center, sloped, color=red] {Start}
    node[midway, above, align=center, sloped, color=red] {Random $\hat{\theta}$};

% Arrows
\draw[->, bend left] (start) to (L);
\draw[->, bend left] (L) to (optimize);
\draw[->, bend left] (optimize) to (new_theta);
\draw[->, bend left] (new_theta) to (start);

\end{tikzpicture}

\caption[Iterative approach to sound design.]{
Iterative approach to sound design. Synthesizer \(g\) with arbitrary initialized parameters \(\hat{\theta}\)
creates sound \(x\). The target sound \(t\) is used as the goal.
Parameters \(\hat{\theta}\) are adjusted to minimize error (or loss)
\(L(\hat{\theta},t)\), where \(L\) is proportional to the difference between
the representations of \(x\) and \(t\). \(\phi\) is the audio feature extractor, or representation function.
}
\label{fig:sound_design_loop_iterative}
\end{figure}



\section{Methodology}
\label{sec:experiment_setup}

\subsection{Experimental Framework}
The iterative optimization loop shown in Fig.~\ref{fig:sound_design_loop_iterative} forms the foundation of all experiments.  
In each trial, a target synthesizer $g_t$ is selected, and a target sound $x = g_t(\theta_t)$ is generated using randomly sampled parameters $\theta_t$.  
An imitator synthesizer $g$ and a loss function $L$ are then chosen, and $g$ is optimized for 200 gradient-based iterations.  In the experiments, either $g_i \neq g_t$ (\todo{refer to experiment sections}) or if $g_i = g_t$, the range of parameter values is non-overlapping. In either case, the output sound cannot precisely match the target sound, corresponding to an out-of-domain (OOD) configuration.

Each \textbf{trial} consists of a single 200-iteration optimization run, resulting in one final \textit{partial parameter loss} (PPL) value.  
An \textbf{experiment} comprises 300 independent trials with the same $(L, g, g_t)$ configuration, yielding a distribution of 300 PPL values.  
A \textbf{scenario} consists of four such experiments—one per loss function—using the same synthesizer pair $(g_t, g)$.  
Comparing the four distributions reveals which loss function best supports imitation for that particular scenario.

The 300 PPL values from each experiment are upsampled by bootstrapping to 1000 samples of the mean, each drawn with replacement from the full distribution~\cite{tibshirani1993introduction,chernick2011bootstrap}.  
The resulting bootstrapped distributions are compared using the non-parametric Scott–Knott (\gls{NPSK}) test~\cite{tantithamthavorn2017mvt,tantithamthavorn2018optimization}, which partitions loss functions into statistically distinct ranks.  
Ranks range from~1 (best-performing) to~4 (worst-performing), with the possibility of ties when distributions are not significantly different.

\subsection{Partial Parameter Loss (PPL)}
In out-of-domain sound-matching, many parameters of the target and imitator synthesizers do not correspond directly—for example, when the target employs a saw oscillator but the imitator uses noise, or when modulation structures differ.  
To evaluate only the aspects of the signal that can meaningfully transfer across domains, we compute a \emph{partial parameter loss} (PPL).  

PPL measures the L1 distance only across the subset of parameters considered perceptually critical for the scenario:
\begin{equation}
\mathrm{PPL} = \| \theta^{(\mathrm{crit})}_t - \hat{\theta}^{(\mathrm{crit})} \|_1,
\end{equation}
where $\theta^{(\mathrm{crit})}$ denotes the target’s critical parameters (e.g., filter cutoffs, modulation rates) and $\hat{\theta}^{(\mathrm{crit})}$ the corresponding recovered parameters of the imitator.  
Parameters with no perceptual or structural correspondence (e.g., oscillator frequency, waveform type) are ignored.  
This enables a fair and interpretable comparison across mismatched synthesizers.

\subsection{Scenario 1: Band-Pass Matching}
The first scenario investigates band-pass filtering in noise and saw-wave synthesizers.  
The target synthesizer $g_t$ is \BPSaw (Listing~\ref{lst:program0_saw}), and the imitator $g$ is \BPNoise (Listing~\ref{lst:program0}).  
The task is defined as reproducing the high-pass and low-pass cutoff frequencies of the target’s band-pass filter, ignoring differences in excitation signal (noise vs. saw).

Since filter cutoffs are directly reflected in the spectral envelope, spectrogram-based loss functions are expected to perform best.  
Indeed, the results in Fig.~\ref{fig:npsk_BP} show that \SIMSESpec\ consistently achieves the lowest PPL, confirming its robustness to amplitude scaling and its suitability for filter-based imitation tasks.

\begin{lstlisting}[caption={\BPNoise}, label={lst:program0}, language=Faust,
float, floatplacement=!H, xleftmargin=1em, xrightmargin=0.5em, firstnumber=0, aboveskip=0em, belowskip=-1em]
import("stdfaust.lib");
lp_cut = hslider("lp_cut",900,100,5000,5);
hp_cut = hslider("hp_cut",100,1,400,5);
process = no.noise:fi.lowpass(3,lp_cut):fi.highpass(10,hp_cut);
\end{lstlisting}

\begin{lstlisting}[caption={\BPSaw}, label={lst:program0_saw}, language=Faust,
float, floatplacement=!H, xleftmargin=1em, xrightmargin=0.5em, firstnumber=0, aboveskip=0em, belowskip=-1em]
import("stdfaust.lib");
lp_cut = hslider("lp_cut", 2801, 100, 5000, 1);
hp_cut = hslider("hp_cut", 142, 1, 400, 1);
sawOsc(f) = +(f/ma.SR) ~ ma.frac;
process = sawOsc(30):fi.lowpass(5, lp_cut):fi.highpass(5, hp_cut);
\end{lstlisting}

\begin{figure}[htbp]
  \centering
  \scriptsize
  \begin{minipage}{\columnwidth}
    \begin{minipage}{0.10\columnwidth}
      \raggedleft
      \vspace{0.5cm}
      SIMSE\\[0.6cm]
      L1\\[0.65cm]
      JTFS\\[0.65cm]
      DTW
    \end{minipage}%
    \begin{minipage}{0.88\columnwidth}
      \centering
      \includegraphics[width=\linewidth]{images/npsk_ood_P_Loss_3.png}
    \end{minipage}
  \end{minipage}
  \caption{Bootstrapped distributions and NPSK ranks for band-pass sound matching.}
  \label{fig:npsk_BP}
\end{figure}

\subsection{Scenario 2: AM Synthesizer Matching}
Amplitude-modulated synthesizers are ideal for evaluating temporal envelope sensitivity, as carrier frequencies and harmonic structures differ significantly across domains.  
We test three variants using the synthesizers \FMMod\ (Listing~\ref{lst:program3}) and \FMModvtwo\ (Listing~\ref{lst:program3_v2}).  
\FMMod\ modulates a saw oscillator’s amplitude with a sinusoidal LFO, while \FMModvtwo\ uses sine oscillators for both carrier and modulator.  
In all cases, the \texttt{amp} parameter (LFO frequency) defines the rate of amplitude modulation and is treated as the critical component for PPL evaluation; the \texttt{car} parameter (carrier frequency) is ignored, as it cannot align across domains.

\begin{lstlisting}[caption={\FMMod}, label={lst:program3},language=Faust,float,floatplacement=!H,xleftmargin=1em,xrightmargin=0.5em,firstnumber=0,aboveskip=0em, belowskip=-1em]
import("stdfaust.lib");
carrier = hslider("car",car_a,car_b,car_c,1);
amp = hslider("amp",amp_a,amp_b,amp_c,1);
sineOsc(f) = +(f/ma.SR) ~ ma.frac:*(2*ma.PI) : sin;
sawOsc(f) = +(f/ma.SR) ~ ma.frac;
process = sineOsc(amp)*sawOsc(car);
\end{lstlisting}

\begin{lstlisting}[caption={\FMModvtwo}, label={lst:program3_v2},language=Faust,float,floatplacement=!H,xleftmargin=1em,xrightmargin=0.5em,firstnumber=0,aboveskip=0em, belowskip=-1em]
import("stdfaust.lib");
carrier = hslider("car",car_a,car_b,car_c,1);
amp = hslider("amp",amp_a,amp_b,amp_c,1);
sineOsc(f) = +(f/ma.SR) ~ ma.frac:*(2*ma.PI) : sin;
process = sineOsc(amp)*sineOsc(car);
\end{lstlisting}

Three OOD conditions are tested:
\begin{enumerate}
  \item \textbf{Non-overlapping carrier frequencies}: target and imitator carriers occupy disjoint frequency ranges (30–250~Hz vs.~1000–5000~Hz).  
  \item \textbf{Sine target, saw imitator}: \FMModvtwo\ as target and \FMMod\ as imitator.  
  \item \textbf{Saw target, sine imitator}: the reverse of condition~2.  
\end{enumerate}

Across all conditions, the DTW\_Envelope loss function performs best (see Fig.~\ref{fig:npsk_am_synths}), reflecting its ability to capture envelope-based temporal similarity despite mismatched spectral structures.

\input{extra_figures/am_scenarios}

\subsection{Scenario 3: Pitch-Bending}
The final set of experiments examines pitch-bending synthesizers, adapted from chirplet-style models used in~\cite{vahidi2023mesostructures}.  
Here, the target and imitator differ by a temporal offset, curvature, or modulation pattern in the pitch trajectory, creating various degrees of OOD mismatch.  
The search parameters include the starting pitch (30–500~Hz) and the exponential rate of pitch increase (1–20 samples per step).  
Three variations are evaluated:
\begin{enumerate}
  \item \textbf{Delayed pitch-bending}: imitator must match a target with a random delay before the bend onset.  
  \item \textbf{No-delay pitch-bending}: same synthesizer pair but without delay (in-domain control).  
  \item \textbf{Pulsating pitch-bending}: adds a periodic modulation on top of the exponential bend to test mesostructural sensitivity.
\end{enumerate}

As shown in Fig.~\ref{fig:npsk_pitch_bends}, \JTFS\ performs well when timing alignment is preserved (scenario~2), but its advantage diminishes when local temporal offsets or composite modulations are introduced.  
DTW\_Envelope outperforms others under delayed and pulsating conditions, confirming its strength in capturing envelope-level temporal similarity.

\input{extra_figures/pitch_bend_scenarios}


\bibliographystyle{alpha}
\bibliography{references}

\end{document}


